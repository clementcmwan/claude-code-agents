#!/bin/bash
# new-ds-project - One-command data science project setup
# Usage: new-ds-project <project-name> [python-version]

set -e  # Exit on error

PROJECT_NAME=$1
PYTHON_VERSION=${2:-3.11}

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Helper functions
print_step() {
    echo -e "${BLUE}==>${NC} $1"
}

print_success() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_error() {
    echo -e "${RED}âœ—${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}âš ${NC} $1"
}

# Check if project name provided
if [ -z "$PROJECT_NAME" ]; then
    print_error "Project name required!"
    echo "Usage: new-ds-project <project-name> [python-version]"
    echo "Example: new-ds-project my-ml-project 3.11"
    exit 1
fi

# Check if directory already exists
if [ -d "$PROJECT_NAME" ]; then
    print_error "Directory '$PROJECT_NAME' already exists!"
    exit 1
fi

echo ""
print_step "ğŸš€ Creating data science project: $PROJECT_NAME"
echo ""

# 1. Create project directory
print_step "ğŸ“ Creating project directory..."
mkdir -p "$PROJECT_NAME"
cd "$PROJECT_NAME"
print_success "Project directory created"

# 2. Create mamba environment
print_step "ğŸ“¦ Creating mamba environment (Python $PYTHON_VERSION)..."
mamba create -n "$PROJECT_NAME" python="$PYTHON_VERSION" -y -q
print_success "Mamba environment created: $PROJECT_NAME"

# 3. Install core data science packages
print_step "ğŸ“š Installing core packages..."
mamba run -n "$PROJECT_NAME" mamba install -y -q \
    numpy \
    pandas \
    scikit-learn \
    matplotlib \
    seaborn \
    jupyter \
    jupyterlab \
    ipykernel \
    notebook
print_success "Core packages installed"

# 4. Install additional useful packages
print_step "ğŸ”§ Installing additional tools..."
mamba run -n "$PROJECT_NAME" mamba install -y -q \
    python-dotenv \
    tqdm \
    requests
print_success "Additional tools installed"

# 5. Install dev tools
print_step "ğŸ› ï¸  Installing dev tools..."
mamba run -n "$PROJECT_NAME" mamba install -y -q \
    pytest \
    black \
    flake8 \
    ipdb
mamba run -n "$PROJECT_NAME" pip install -q autoflake sqlfluff
print_success "Dev tools installed"

# 6. Install MLflow (via pip as it's not always in conda)
print_step "ğŸ“Š Installing MLflow..."
mamba run -n "$PROJECT_NAME" pip install -q mlflow
print_success "MLflow installed"

# 7. Create project structure
print_step "ğŸ“‚ Creating project structure..."
mkdir -p data/{raw,processed,external}
mkdir -p notebooks
mkdir -p src
mkdir -p models
mkdir -p logs
mkdir -p reports/{figures,tables}
mkdir -p tests
mkdir -p config

# Create .gitkeep files
touch data/raw/.gitkeep
touch data/processed/.gitkeep
touch data/external/.gitkeep
touch models/.gitkeep
touch logs/.gitkeep
touch reports/figures/.gitkeep
touch reports/tables/.gitkeep
print_success "Project structure created"

# 8. Create .gitignore
print_step "ğŸ“ Creating .gitignore..."
cat > .gitignore <<'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
env/
venv/
ENV/
env.bak/
venv.bak/

# Jupyter Notebook
.ipynb_checkpoints/
*.ipynb_checkpoints

# Data
data/raw/*
data/processed/*
data/external/*
!data/raw/.gitkeep
!data/processed/.gitkeep
!data/external/.gitkeep

# Models
models/*
!models/.gitkeep

# Logs
logs/*
!logs/.gitkeep
*.log

# Reports
reports/figures/*
reports/tables/*
!reports/figures/.gitkeep
!reports/tables/.gitkeep

# MLflow
mlruns/
mlartifacts/

# Environment variables
.env
.env.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# pytest
.pytest_cache/
.coverage
htmlcov/

# mypy
.mypy_cache/
.dmypy.json
dmypy.json
EOF
print_success ".gitignore created"

# 9. Create environment.yml
print_step "ğŸ“‹ Creating environment.yml..."
cat > environment.yml <<EOF
name: $PROJECT_NAME
channels:
  - conda-forge
  - defaults
dependencies:
  - python=$PYTHON_VERSION
  - numpy
  - pandas
  - scikit-learn
  - matplotlib
  - seaborn
  - jupyter
  - jupyterlab
  - ipykernel
  - notebook
  - python-dotenv
  - tqdm
  - requests
  - pytest
  - black
  - flake8
  - ipdb
  - pip
  - pip:
      - mlflow
      - sqlfluff
EOF
print_success "environment.yml created"

# 10. Create requirements.txt
print_step "ğŸ“‹ Creating requirements.txt..."
mamba run -n "$PROJECT_NAME" pip freeze > requirements.txt
print_success "requirements.txt created"

# 11. Create .env.example
print_step "ğŸ” Creating .env.example..."
cat > .env.example <<'EOF'
# Environment Variables Template
# Copy this file to .env and fill in your values

# MLflow
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=default

# Logging
LOG_LEVEL=INFO

# Paths
DATA_DIR=./data
MODEL_DIR=./models
LOG_DIR=./logs

# API Keys (never commit actual keys!)
# API_KEY=your_api_key_here
EOF
print_success ".env.example created"

# 12. Create README.md
print_step "ğŸ“– Creating README.md..."
cat > README.md <<EOF
# $PROJECT_NAME

Data science project created with \`new-ds-project\`.

## Setup

### Prerequisites
- Miniforge/Mamba installed
- Python $PYTHON_VERSION

### Installation

\`\`\`bash
# Create environment from environment.yml
mamba env create -f environment.yml

# Or create from requirements.txt
mamba create -n $PROJECT_NAME python=$PYTHON_VERSION
mamba activate $PROJECT_NAME
pip install -r requirements.txt
\`\`\`

### Activate Environment

\`\`\`bash
mamba activate $PROJECT_NAME
\`\`\`

## Project Structure

\`\`\`
$PROJECT_NAME/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Original, immutable data
â”‚   â”œâ”€â”€ processed/        # Cleaned, transformed data
â”‚   â””â”€â”€ external/         # External data sources
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”‚   â””â”€â”€ exploratory/      # EDA notebooks
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ data/             # Data loading and processing
â”‚   â”œâ”€â”€ features/         # Feature engineering
â”‚   â”œâ”€â”€ models/           # Model training and prediction
â”‚   â””â”€â”€ visualization/    # Visualization scripts
â”œâ”€â”€ models/               # Trained models
â”œâ”€â”€ logs/                 # Log files
â”œâ”€â”€ reports/              # Generated reports
â”‚   â”œâ”€â”€ figures/          # Figures for reports
â”‚   â””â”€â”€ tables/           # Tables for reports
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ environment.yml       # Conda environment specification
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
\`\`\`

## Usage

### Jupyter Lab

\`\`\`bash
mamba activate $PROJECT_NAME
jupyter lab
\`\`\`

### MLflow Tracking

\`\`\`bash
# Start MLflow UI
mlflow ui --port 5000

# Access at: http://localhost:5000
\`\`\`

### Running Scripts

\`\`\`bash
mamba activate $PROJECT_NAME
python src/train.py
\`\`\`

### Testing

\`\`\`bash
pytest tests/
\`\`\`

### Code Formatting

\`\`\`bash
# Format Python code with black
black src/ tests/

# Format SQL files with sqlfluff
sqlfluff fix --dialect postgres *.sql

# Check code style with flake8
flake8 src/ tests/
\`\`\`

## Development Workflow

1. **Explore data** in \`notebooks/exploratory/\`
2. **Develop features** in \`src/features/\`
3. **Train models** in \`src/models/\`
4. **Track experiments** with MLflow
5. **Write tests** in \`tests/\`
6. **Document** findings in \`reports/\`

## Environment Variables

Copy \`.env.example\` to \`.env\` and configure:

\`\`\`bash
cp .env.example .env
# Edit .env with your values
\`\`\`

## Git Workflow

\`\`\`bash
# Initialize git (if not already done)
git init

# Add files
git add .

# Commit
git commit -m "Initial commit"

# Add remote
git remote add origin <your-repo-url>

# Push
git push -u origin main
\`\`\`

## Updating Environment

\`\`\`bash
# Add new package
mamba install <package-name>

# Update environment.yml
mamba env export > environment.yml

# Update requirements.txt
pip freeze > requirements.txt
\`\`\`

## Deactivate Environment

\`\`\`bash
mamba deactivate
\`\`\`

## Remove Environment

\`\`\`bash
mamba env remove -n $PROJECT_NAME
\`\`\`

---

**Created:** $(date +%Y-%m-%d)
**Python Version:** $PYTHON_VERSION
**Environment:** $PROJECT_NAME
EOF
print_success "README.md created"

# 13. Create src structure
print_step "ğŸ—ï¸  Creating src structure..."
mkdir -p src/{data,features,models,visualization}

# Create __init__.py files
touch src/__init__.py
touch src/data/__init__.py
touch src/features/__init__.py
touch src/models/__init__.py
touch src/visualization/__init__.py

# Create example files
cat > src/data/load_data.py <<'EOF'
"""
Data loading utilities
"""
import pandas as pd
from pathlib import Path


def load_raw_data(filename: str) -> pd.DataFrame:
    """
    Load raw data from data/raw directory.
    
    Args:
        filename: Name of the file to load
        
    Returns:
        DataFrame with loaded data
    """
    data_path = Path(__file__).parents[2] / "data" / "raw" / filename
    df = pd.read_csv(data_path)
    return df


def save_processed_data(df: pd.DataFrame, filename: str) -> None:
    """
    Save processed data to data/processed directory.
    
    Args:
        df: DataFrame to save
        filename: Name of the file to save
    """
    data_path = Path(__file__).parents[2] / "data" / "processed" / filename
    df.to_csv(data_path, index=False)
EOF

cat > src/models/train.py <<'EOF'
"""
Model training script
"""
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def train_model(X, y, **params):
    """
    Train a model with MLflow tracking.
    
    Args:
        X: Features
        y: Target
        **params: Model parameters
        
    Returns:
        Trained model
    """
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # Start MLflow run
    with mlflow.start_run():
        # Log parameters
        mlflow.log_params(params)
        
        # Train model
        logger.info("Training model...")
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # Log metrics
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("f1_score", f1)
        
        logger.info(f"Accuracy: {accuracy:.4f}")
        logger.info(f"F1 Score: {f1:.4f}")
        
        # Log model
        mlflow.sklearn.log_model(model, "model")
        
    return model


if __name__ == "__main__":
    # Example usage
    from sklearn.datasets import make_classification
    
    X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
    
    params = {
        "n_estimators": 100,
        "max_depth": 10,
        "random_state": 42
    }
    
    model = train_model(X, y, **params)
EOF

print_success "src structure created with example files"

# 14. Create example notebook
print_step "ğŸ““ Creating example notebook..."
mkdir -p notebooks/exploratory
cat > notebooks/exploratory/01_initial_exploration.ipynb <<'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration\n",
    "\n",
    "This notebook contains initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# df = pd.read_csv('../data/raw/your_data.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your visualizations here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF
print_success "Example notebook created"

# 15. Create pytest configuration
print_step "ğŸ§ª Creating pytest configuration..."
cat > pytest.ini <<'EOF'
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
EOF

# Create example test
cat > tests/test_example.py <<'EOF'
"""
Example test file
"""
import pytest


def test_example():
    """Example test that always passes."""
    assert 1 + 1 == 2


def test_imports():
    """Test that core packages can be imported."""
    import numpy as np
    import pandas as pd
    import sklearn
    
    assert np.__version__
    assert pd.__version__
    assert sklearn.__version__
EOF

print_success "pytest configuration created"

# 16. Initialize git
print_step "ğŸ”§ Initializing git repository..."
git init -q
git add .
git commit -q -m "Initial commit: Project setup with new-ds-project"
print_success "Git repository initialized"

# 16b. Install git hooks
HOOKS_DIR="$(dirname "$(readlink -f "$0")")/hooks"
if [ -d "$HOOKS_DIR" ]; then
    print_step "ğŸª Installing git hooks..."
    cp "$HOOKS_DIR/pre-commit" .git/hooks/pre-commit 2>/dev/null && chmod +x .git/hooks/pre-commit
    cp "$HOOKS_DIR/commit-msg" .git/hooks/commit-msg 2>/dev/null && chmod +x .git/hooks/commit-msg
    cp "$HOOKS_DIR/pre-push" .git/hooks/pre-push 2>/dev/null && chmod +x .git/hooks/pre-push
    cp "$HOOKS_DIR/post-merge" .git/hooks/post-merge 2>/dev/null && chmod +x .git/hooks/post-merge
    print_success "Git hooks installed (pre-commit, commit-msg, pre-push, post-merge)"
fi

# 17. Register Jupyter kernel
print_step "ğŸ““ Registering Jupyter kernel..."
mamba run -n "$PROJECT_NAME" python -m ipykernel install --user --name="$PROJECT_NAME" --display-name="Python ($PROJECT_NAME)" > /dev/null 2>&1
print_success "Jupyter kernel registered"

# 18. Create quick start script
print_step "ğŸš€ Creating quick start script..."
cat > start.sh <<'SCRIPT_EOF'
#!/bin/bash
# Quick start script for PROJECT_NAME_PLACEHOLDER

echo "ğŸš€ Starting PROJECT_NAME_PLACEHOLDER environment..."
echo "ğŸ““ Starting Jupyter Lab..."
echo "   Access at: http://localhost:8888"
echo ""

# Run Jupyter Lab in the mamba environment
mamba run -n PROJECT_NAME_PLACEHOLDER jupyter lab
SCRIPT_EOF

# Replace placeholder with actual project name
sed -i '' "s/PROJECT_NAME_PLACEHOLDER/$PROJECT_NAME/g" start.sh
chmod +x start.sh
print_success "Quick start script created"

# 19. Summary
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
print_success "âœ¨ Project '$PROJECT_NAME' created successfully!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“¦ Environment: $PROJECT_NAME (Python $PYTHON_VERSION)"
echo "ğŸ“ Location: $(pwd)"
echo ""
echo "ğŸš€ Quick Start:"
echo ""
echo "   cd $PROJECT_NAME"
echo "   mamba activate $PROJECT_NAME"
echo "   jupyter lab"
echo ""
echo "   Or simply run:"
echo "   ./start.sh"
echo ""
echo "ğŸ“š Installed Packages:"
echo "   â€¢ NumPy, Pandas, Scikit-learn"
echo "   â€¢ Matplotlib, Seaborn"
echo "   â€¢ Jupyter Lab"
echo "   â€¢ MLflow"
echo "   â€¢ pytest, black, flake8, sqlfluff"
echo ""
echo "ğŸ“– Next Steps:"
echo "   1. Copy .env.example to .env and configure"
echo "   2. Add your data to data/raw/"
echo "   3. Start exploring in notebooks/"
echo "   4. Develop code in src/"
echo "   5. Track experiments with MLflow"
echo ""
echo "ğŸ’¡ Useful Commands:"
echo "   mamba activate $PROJECT_NAME    # Activate environment"
echo "   mamba deactivate                # Deactivate environment"
echo "   mamba install <package>         # Install new package"
echo "   jupyter lab                     # Start Jupyter Lab"
echo "   mlflow ui                       # Start MLflow UI"
echo "   pytest                          # Run tests"
echo "   black src/                      # Format code"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
